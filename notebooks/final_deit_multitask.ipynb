{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Final Model — Multi-Head DeiT-Base (Training + Evaluation)\n",
        "\n",
        "This notebook contains the code used to train and evaluate our **final model**:\n",
        "**DeiT-Base multi-head** (one classification head per behavior attribute).\n",
        "\n",
        "**Assumptions (same as the original run):**\n",
        "- Dataset path in Drive: `MyDrive/dataset_project`\n",
        "- A labels CSV exists: `labels.csv`\n",
        "- `labels.csv` includes at least: `filename`, `split`\n",
        "- Label columns (as used in the code): `Gaze`, `Headphones`, `Environment`, `Privacy`, `ObjectInHand`\n",
        "- Split folders exist under `dataset_project`: `train/`, `validation/`, `test/`\n",
        "\n",
        "**Main outputs:**\n",
        "- Model checkpoints: `models/train_last.pth`, `models/val_best.pth`\n",
        "- Test metrics CSV: `models/test_results.csv`\n",
        "- Figures (optional): saved under `plots/` and `baseline_results/`"
      ],
      "metadata": {
        "id": "lZIA1pq6gESr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Setup\n",
        "\n",
        "Install dependencies, mount Drive, and define:\n",
        "- Paths\n",
        "- Label maps\n",
        "- Dataset class\n",
        "- Multi-head DeiT model"
      ],
      "metadata": {
        "id": "_Tc2zLwHxPm6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bG57o9AeSXH"
      },
      "outputs": [],
      "source": [
        "!pip -q install timm\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch, torch.nn as nn\n",
        "import timm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "DATASET_DIR = Path(\"/content/drive/MyDrive/dataset_project\")\n",
        "CSV_PATH = DATASET_DIR / \"labels.csv\"\n",
        "\n",
        "TRAIN_DIR = DATASET_DIR / \"train\"\n",
        "VAL_DIR   = DATASET_DIR / \"validation\"\n",
        "TEST_DIR  = DATASET_DIR / \"test\"\n",
        "\n",
        "MODELS_DIR = DATASET_DIR / \"models\"\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "LABEL_MAPS = {\n",
        "    \"gaze\": {\"camera\":0, \"not_camera\":1, \"not camera\":1, \"eyes_closed\":2, \"eyes close\":2},\n",
        "    \"headphones\": {\"with_headphones\":0,\"with headphones\":0,\"with_headphone\":0,\n",
        "                   \"without_headphones\":1,\"without headphones\":1,\"without_headphone\":1,\n",
        "                   \"unknown\":2},\n",
        "    \"environment\": {\"indoor\":0, \"outdoor\":1},\n",
        "    \"privacy\": {\"private\":0, \"public\":1},\n",
        "    \"object\": {\"cup\":0, \"phone\":1, \"pen\":2, \"none\":3, \"other\":4, \"unknown\":5}\n",
        "}\n",
        "NUM_CLASSES = {\"gaze\":3,\"headphones\":3,\"environment\":2,\"privacy\":2,\"object\":6}\n",
        "\n",
        "CSV_COLS = {\"gaze\":\"Gaze\",\"headphones\":\"Headphones\",\"environment\":\"Environment\",\"privacy\":\"Privacy\",\"object\":\"ObjectInHand\"}\n",
        "\n",
        "def clean_label(v):\n",
        "    if pd.isna(v): return \"\"\n",
        "    return str(v).strip().lower()\n",
        "\n",
        "def resolve_image_path(split_value, filename):\n",
        "    split = str(split_value).strip().lower()\n",
        "    folder = TRAIN_DIR if split==\"train\" else VAL_DIR if split in [\"val\",\"validation\"] else TEST_DIR if split==\"test\" else DATASET_DIR\n",
        "    fname = str(filename).replace(\"\\\\\",\"/\").strip().split(\"/\")[-1]\n",
        "\n",
        "    p = folder / fname\n",
        "    if p.exists(): return p\n",
        "\n",
        "    stem = Path(fname).stem\n",
        "    for ext in (\".jpg\",\".jpeg\",\".png\",\".webp\"):\n",
        "        cand = folder / f\"{stem}{ext}\"\n",
        "        if cand.exists(): return cand\n",
        "    return None\n",
        "\n",
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.transform = transform\n",
        "        paths, rows = [], []\n",
        "        for _, r in df.iterrows():\n",
        "            p = resolve_image_path(r[\"split\"], r[\"filename\"])\n",
        "            if p is not None:\n",
        "                paths.append(p)\n",
        "                rows.append(r)\n",
        "        self.df = pd.DataFrame(rows).reset_index(drop=True)\n",
        "        self.paths = paths\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        if self.transform: img = self.transform(img)\n",
        "\n",
        "        row = self.df.iloc[idx]\n",
        "        targets = {}\n",
        "        defaults = {\"gaze\":\"camera\",\"environment\":\"indoor\",\"privacy\":\"private\"}\n",
        "        for task, col in CSV_COLS.items():\n",
        "            val = clean_label(row[col])\n",
        "            if val not in LABEL_MAPS[task]:\n",
        "                val = \"unknown\" if \"unknown\" in LABEL_MAPS[task] else defaults[task]\n",
        "            targets[task] = torch.tensor(LABEL_MAPS[task][val], dtype=torch.long)\n",
        "        return img, targets\n",
        "\n",
        "class MultiHeadDeiT(nn.Module):\n",
        "    def __init__(self, backbone_name, num_classes, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(backbone_name, pretrained=pretrained, num_classes=0)\n",
        "        dim = self.backbone.num_features\n",
        "        self.heads = nn.ModuleDict({t: nn.Linear(dim, n) for t, n in num_classes.items()})\n",
        "    def forward(self, x):\n",
        "        feat = self.backbone(x)\n",
        "        return {t: head(feat) for t, head in self.heads.items()}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load CSV + Build DataLoaders\n",
        "\n",
        "- Read `labels.csv`\n",
        "- Split by `split` column (train / val / test)\n",
        "- Create transforms\n",
        "- Build DataLoaders"
      ],
      "metadata": {
        "id": "DFKomY62gN8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(CSV_PATH)\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "df_train = df[df[\"split\"].astype(str).str.strip().str.lower()==\"train\"].reset_index(drop=True)\n",
        "df_val   = df[df[\"split\"].astype(str).str.strip().str.lower().isin([\"val\",\"validation\"])].reset_index(drop=True)\n",
        "df_test  = df[df[\"split\"].astype(str).str.strip().str.lower()==\"test\"].reset_index(drop=True)\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "eval_tf = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "train_ds = MultiTaskDataset(df_train, transform=train_tf)\n",
        "val_ds   = MultiTaskDataset(df_val,   transform=eval_tf)\n",
        "test_ds  = MultiTaskDataset(df_test,  transform=eval_tf)\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "val_dl   = DataLoader(val_ds,   batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_dl  = DataLoader(test_ds,  batch_size=32, shuffle=False, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "30LxbTQKeUcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Train DeiT Multi-Head (15 epochs)\n",
        "\n",
        "- Pretrained DeiT backbone\n",
        "- Multi-head classification (5 tasks)\n",
        "- Loss = sum of CrossEntropy across heads\n",
        "- Saves last checkpoint: `models/train_last.pth`"
      ],
      "metadata": {
        "id": "YD7Jq1F9gTlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiHeadDeiT(\"deit_base_patch16_224\", NUM_CLASSES, pretrained=True).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.05)\n",
        "\n",
        "use_amp = torch.cuda.is_available()\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "EPOCHS = 15\n",
        "TRAIN_SAVE = MODELS_DIR / \"train_last.pth\"\n",
        "\n",
        "model.train()\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    total = 0.0\n",
        "    pbar = tqdm(train_dl, desc=f\"Train {epoch}/{EPOCHS}\")\n",
        "    for imgs, targets in pbar:\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        targets = {k:v.to(device, non_blocking=True) for k,v in targets.items()}\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            out = model(imgs)\n",
        "            loss = sum(criterion(out[t], targets[t]) for t in out)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total += loss.item()\n",
        "        pbar.set_postfix(loss=total/max(1,pbar.n))\n",
        "\n",
        "    torch.save(model.state_dict(), TRAIN_SAVE)\n",
        "print(\"Saved:\", TRAIN_SAVE)"
      ],
      "metadata": {
        "id": "6BEaVwGQgYgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Validation pass + Save best checkpoint\n",
        "\n",
        "Evaluates validation loss and saves best weights to: `models/val_best.pth`"
      ],
      "metadata": {
        "id": "dnmEq58ygcqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VAL_BEST = MODELS_DIR / \"val_best.pth\"\n",
        "val_best_loss = float(\"inf\")\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    pbar = tqdm(val_dl, desc=\"Validation\")\n",
        "    for imgs, targets in pbar:\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        targets = {k:v.to(device, non_blocking=True) for k,v in targets.items()}\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            out = model(imgs)\n",
        "            loss = sum(criterion(out[t], targets[t]) for t in out)\n",
        "        val_loss += loss.item()\n",
        "        pbar.set_postfix(loss=val_loss/max(1,pbar.n))\n",
        "\n",
        "avg_val = val_loss / max(1, len(val_dl))\n",
        "if avg_val < val_best_loss:\n",
        "    val_best_loss = avg_val\n",
        "    torch.save(model.state_dict(), VAL_BEST)\n",
        "\n",
        "print(\"avg_val_loss:\", avg_val)\n",
        "print(\"best_path:\", VAL_BEST, \"best_loss:\", val_best_loss)"
      ],
      "metadata": {
        "id": "d6SYe7wrgfGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Test evaluation (Accuracy per task)\n",
        "\n",
        "Loads `val_best.pth` (if exists) and prints per-task test accuracy."
      ],
      "metadata": {
        "id": "1QZT5cRJghQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# טוענים את best של הולידציה אם קיים\n",
        "BEST = MODELS_DIR / \"val_best.pth\"\n",
        "if BEST.exists():\n",
        "    model.load_state_dict(torch.load(BEST, map_location=device))\n",
        "\n",
        "model.eval()\n",
        "\n",
        "correct = {t:0 for t in NUM_CLASSES}\n",
        "total   = {t:0 for t in NUM_CLASSES}\n",
        "\n",
        "with torch.no_grad():\n",
        "    pbar = tqdm(test_dl, desc=\"Test\")\n",
        "    for imgs, targets in pbar:\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        targets = {k:v.to(device, non_blocking=True) for k,v in targets.items()}\n",
        "        out = model(imgs)\n",
        "\n",
        "        for t in out:\n",
        "            pred = out[t].argmax(dim=1)\n",
        "            correct[t] += (pred == targets[t]).sum().item()\n",
        "            total[t] += targets[t].numel()\n",
        "\n",
        "for t in NUM_CLASSES:\n",
        "    acc = correct[t] / max(1, total[t])\n",
        "    print(t, \"acc:\", acc)"
      ],
      "metadata": {
        "id": "F253Rux5gjTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Export test results to CSV\n",
        "\n",
        "Saves: `models/test_results.csv`"
      ],
      "metadata": {
        "id": "pK4IkavZgmpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_path = MODELS_DIR / \"test_results.csv\"\n",
        "rows = []\n",
        "for t in NUM_CLASSES:\n",
        "    rows.append({\"task\": t, \"accuracy\": correct[t]/max(1,total[t]), \"correct\": correct[t], \"total\": total[t]})\n",
        "\n",
        "pd.DataFrame(rows).to_csv(results_path, index=False)\n",
        "print(\"Saved:\", results_path)"
      ],
      "metadata": {
        "id": "G2wF_x27go5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Macro-F1 + Overall metrics (Mean Accuracy + Joint Accuracy)\n",
        "\n",
        "Collect predictions on the test set and compute:\n",
        "- Accuracy per task\n",
        "- Macro-F1 per task\n",
        "- Mean Accuracy (avg over tasks)\n",
        "- Joint Accuracy (all 5 tasks correct)"
      ],
      "metadata": {
        "id": "kMx_ddHLgrjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install scikit-learn\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "TASKS = [\"gaze\", \"headphones\", \"environment\", \"privacy\", \"object\"]\n",
        "\n",
        "model.eval()\n",
        "\n",
        "y_true = {t: [] for t in TASKS}\n",
        "y_pred = {t: [] for t in TASKS}\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, targets in test_dl:\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        targets = {k: v.to(device, non_blocking=True) for k, v in targets.items()}\n",
        "\n",
        "        out = model(imgs)\n",
        "        for t in TASKS:\n",
        "            pred = out[t].argmax(dim=1)\n",
        "            y_true[t].append(targets[t].cpu().numpy())\n",
        "            y_pred[t].append(pred.cpu().numpy())\n",
        "\n",
        "for t in TASKS:\n",
        "    y_true[t] = np.concatenate(y_true[t])\n",
        "    y_pred[t] = np.concatenate(y_pred[t])\n",
        "\n",
        "print(\"Done collecting predictions.\")"
      ],
      "metadata": {
        "id": "rhnk_XoVgtlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = {}\n",
        "f1m = {}\n",
        "\n",
        "for t in TASKS:\n",
        "    acc[t] = float((y_pred[t] == y_true[t]).mean())\n",
        "    f1m[t] = float(f1_score(y_true[t], y_pred[t], average=\"macro\"))\n",
        "\n",
        "# Accuracy כולל יחד: תמונה נחשבת נכונה רק אם כל ה-5 משימות נכונות\n",
        "all_correct = np.ones_like(y_true[TASKS[0]], dtype=bool)\n",
        "for t in TASKS:\n",
        "    all_correct &= (y_pred[t] == y_true[t])\n",
        "joint_accuracy = float(all_correct.mean())\n",
        "\n",
        "avg_accuracy = float(np.mean([acc[t] for t in TASKS]))\n",
        "\n",
        "print(\"=== Accuracy per task ===\")\n",
        "for t in TASKS:\n",
        "    print(f\"{t}: {acc[t]:.4f}\")\n",
        "\n",
        "print(\"\\n=== Macro F1 per task ===\")\n",
        "for t in TASKS:\n",
        "    print(f\"{t}: {f1m[t]:.4f}\")\n",
        "\n",
        "print(\"\\n=== Overall ===\")\n",
        "print(\"Average accuracy (mean over tasks):\", f\"{avg_accuracy:.4f}\")\n",
        "print(\"Joint accuracy (all tasks correct):\", f\"{joint_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "Q-DbMLRwgx9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Confusion matrix (default: Object-in-hand)\n",
        "\n",
        "Change `TASK_CM` if you want a different task confusion matrix."
      ],
      "metadata": {
        "id": "MSL-tNaig02s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TASK_CM = \"object\"  # אפשר לשנות ל: \"gaze\" / \"headphones\" / \"environment\" / \"privacy\" / \"object\"\n",
        "\n",
        "idx_to_label = {v: k for k, v in LABEL_MAPS[TASK_CM].items()}\n",
        "labels_order = [idx_to_label[i] for i in range(NUM_CLASSES[TASK_CM])]\n",
        "\n",
        "cm = confusion_matrix(y_true[TASK_CM], y_pred[TASK_CM], labels=list(range(NUM_CLASSES[TASK_CM])))\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_order)\n",
        "plt.figure(figsize=(7, 7))\n",
        "disp.plot(values_format=\"d\", xticks_rotation=45)\n",
        "plt.title(f\"Confusion Matrix - {TASK_CM}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Lljofkv-g4E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Export plots used in the presentation\n",
        "\n",
        "⚠️ This section uses the hard-coded results (exactly as in the original run),\n",
        "so the exported plots match the reported numbers."
      ],
      "metadata": {
        "id": "8oxhZ37gg60H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# איפה לשמור את התמונות (בדרייב)\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/dataset_project/plots\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# התוצאות שלך\n",
        "acc = {\n",
        "    \"gaze\": 0.5575,\n",
        "    \"headphones\": 0.5977,\n",
        "    \"environment\": 0.8908,\n",
        "    \"privacy\": 0.9655,\n",
        "    \"object\": 0.6897\n",
        "}\n",
        "\n",
        "f1 = {\n",
        "    \"gaze\": 0.4357,\n",
        "    \"headphones\": 0.5699,\n",
        "    \"environment\": 0.4711,\n",
        "    \"privacy\": 0.4912,\n",
        "    \"object\": 0.3442\n",
        "}\n",
        "\n",
        "overall_mean_acc = 0.7402\n",
        "joint_acc = 0.2471\n",
        "\n",
        "print(\"✅ OUT_DIR:\", OUT_DIR)"
      ],
      "metadata": {
        "id": "tMuj-yoQg_Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tasks = list(acc.keys())\n",
        "x = np.arange(len(tasks))\n",
        "width = 0.38\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(x - width/2, [acc[t] for t in tasks], width=width, label=\"Accuracy\", color=\"#1f77b4\")\n",
        "plt.bar(x + width/2, [f1[t] for t in tasks],  width=width, label=\"Macro F1\", color=\"#6baed6\")\n",
        "\n",
        "plt.ylim(0, 1.05)\n",
        "plt.xticks(x, tasks, rotation=0)\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Per-Task Performance (Accuracy & Macro F1)\")\n",
        "plt.grid(axis=\"y\", alpha=0.25)\n",
        "plt.legend()\n",
        "\n",
        "path = OUT_DIR / \"per_task_accuracy_f1.png\"\n",
        "plt.tight_layout()\n",
        "plt.savefig(path, dpi=300)\n",
        "plt.show()\n",
        "\n",
        "print(\"✅ Saved:\", path)"
      ],
      "metadata": {
        "id": "RdwI1ijphC7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "labels = [\"Mean Accuracy\\n(mean over tasks)\", \"Joint Accuracy\\n(all tasks correct)\"]\n",
        "vals = [overall_mean_acc, joint_acc]\n",
        "\n",
        "plt.bar(labels, vals, color=[\"#1f77b4\", \"#3182bd\"])\n",
        "plt.ylim(0, 1.05)\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Overall Performance\")\n",
        "plt.grid(axis=\"y\", alpha=0.25)\n",
        "\n",
        "for i, v in enumerate(vals):\n",
        "    plt.text(i, v + 0.02, f\"{v:.3f}\", ha=\"center\", fontweight=\"bold\")\n",
        "\n",
        "path = OUT_DIR / \"overall_mean_vs_joint.png\"\n",
        "plt.tight_layout()\n",
        "plt.savefig(path, dpi=300)\n",
        "plt.show()\n",
        "\n",
        "print(\"✅ Saved:\", path)"
      ],
      "metadata": {
        "id": "vTNRC8vDyBbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. (Optional) Baseline comparison figure\n",
        "\n",
        "Compares baseline train/val behavior to our final DeiT test average accuracy line\n",
        "(uses `baseline_results/baseline_metrics.csv`)."
      ],
      "metadata": {
        "id": "g9lWenlshE3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "BASELINE_DIR = Path(\"/content/drive/MyDrive/dataset_project/baseline_results\")\n",
        "METRICS_PATH = BASELINE_DIR / \"baseline_metrics.csv\"\n",
        "\n",
        "# ממכם:\n",
        "NEW_AVG_ACC = 0.7402  # Average accuracy (mean over tasks)\n",
        "\n",
        "dfm = pd.read_csv(METRICS_PATH)\n",
        "\n",
        "epochs = dfm[\"Epoch\"]\n",
        "train_loss = dfm[\"Train_Loss\"]\n",
        "val_avg = dfm[\"Val_Avg_Accuracy\"]\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "ax1 = plt.gca()\n",
        "ax1.plot(epochs, train_loss, marker=\"o\", label=\"Baseline Train Loss\", color=plt.cm.Blues(0.75))\n",
        "ax1.set_xlabel(\"Epoch\")\n",
        "ax1.set_ylabel(\"Train Loss\")\n",
        "ax1.grid(alpha=0.25)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(epochs, val_avg, marker=\"s\", label=\"Baseline Val Avg Accuracy\", color=plt.cm.Blues(0.45))\n",
        "ax2.axhline(NEW_AVG_ACC, linestyle=\"--\", label=\"Multi-Head DeiT Test Avg Acc (0.7402)\", color=plt.cm.Blues(0.9))\n",
        "ax2.set_ylabel(\"Validation Avg Accuracy\")\n",
        "\n",
        "lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "plt.legend(lines1 + lines2, labels1 + labels2, loc=\"lower right\")\n",
        "\n",
        "plt.title(\"Baseline Overfitting: Train Loss vs Validation Accuracy (+ Our Final Test Avg Acc)\")\n",
        "out_path = BASELINE_DIR / \"baseline_overfitting_plus_ours.png\"\n",
        "plt.tight_layout()\n",
        "plt.savefig(out_path, dpi=200)\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved:\", out_path)"
      ],
      "metadata": {
        "id": "niV5Vet8hHGE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}